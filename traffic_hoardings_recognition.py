# -*- coding: utf-8 -*-
"""Traffic Hoardings Recognition.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1lwI06NDxNlSxadF3GH9O_e3ioW89OQta

**Mounting the Drive to extract DataSets from Drive**
"""

from google.colab import drive
drive.mount('/content/drive')

"""**Importing and Installing the required Modules**

"""

!pip install keras-tuner -q
import keras_tuner
import numpy as np 
import pandas as pd 
import pickle
import matplotlib.pyplot as plt
import tensorflow as tf

from tensorflow import keras
from timeit import default_timer as timer
from keras.utils.np_utils import to_categorical
from keras.models import Sequential
from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D, AvgPool2D, BatchNormalization, Reshape
from keras.preprocessing.image import ImageDataGenerator
from keras.callbacks import LearningRateScheduler
from kerastuner import RandomSearch
from kerastuner.engine.hyperparameters import HyperParameters

# Unzipping the Datasets from Drive
!unzip /content/drive/MyDrive/Project/archive.zip

"""**Loading RGB Examples from Dataset data2.pickle**"""

# Opening the file & Reading the binary mode
with open('/content/data2.pickle', 'rb') as f:
    data = pickle.load(f, encoding='latin1')  

# Preparing y_train and y_validation for using in Keras
data['y_train'] = to_categorical(data['y_train'], num_classes=43)
data['y_validation'] = to_categorical(data['y_validation'], num_classes=43)

# Making channels come at the end
data['x_train'] = data['x_train'].transpose(0, 2, 3, 1)
data['x_validation'] = data['x_validation'].transpose(0, 2, 3, 1)
data['x_test'] = data['x_test'].transpose(0, 2, 3, 1)

# Showing loaded data from file
for i, j in data.items():
    if i == 'labels':
        print(i + ':', len(j))
    else: 
        print(i + ':', j.shape)

"""**Visualising Data from Training Dataset**"""

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline

# Preparing function for ploting set of examples
# As input it will take 4D tensor and convert it to the grid
# Values will be scaled to the range [0, 255]
def convert_to_grid(x_input):
    N, H, W, C = x_input.shape
    grid_size = int(np.ceil(np.sqrt(N)))
    grid_height = H * grid_size + 1 * (grid_size - 1)
    grid_width = W * grid_size + 1 * (grid_size - 1)
    grid = np.zeros((grid_height, grid_width, C)) + 255
    next_idx = 0
    y0, y1 = 0, H
    for y in range(grid_size):
        x0, x1 = 0, W
        for x in range(grid_size):
            if next_idx < N:
              img = x_input[next_idx]
              low, high = np.min(img), np.max(img)
              grid[y0:y1, x0:x1] = 255.0 * (img - low) / (high - low)
              next_idx += 1
            x0 += W + 1
            x1 += W + 1
        y0 += H + 1
        y1 += H + 1

    return grid


# Visualizing some examples of training data
examples = data['x_train'][86889:86989, :, :, :]
print(examples.shape)  # (81, 32, 32, 3)

# Plotting some examples
fig = plt.figure()
grid = convert_to_grid(examples)
plt.imshow(grid.astype('uint8'), cmap='gray')
plt.axis('off')
plt.gcf().set_size_inches(15, 15)
plt.title('Some examples of training data', fontsize=18)

# Showing the plot
plt.show()

# Saving the plot
fig.savefig('training_examples.png')
plt.close()

"""**Importing Keras Tuner Module**"""

from kerastuner import RandomSearch
from kerastuner.engine.hyperparameters import HyperParameters
from keras_tuner import HyperModel

"""**Preparing Model-Building Function**

**Hyperparameter tuning**
"""

filter_size =  [3,5,7,9,11,13,15,19]
model = [0]*len(filter_size)

class MyHyperModel(HyperModel):
    def __init__(self, classes):
        self.classes = classes


    def build(self, hp):
      model[self.classes] = keras.Sequential([
        keras.layers.Conv2D(
            filters=32,
            kernel_size=filter_size[self.classes],
            activation='relu',
            padding = 'same',
            input_shape=(32,32,3)
        ),
        keras.layers.Conv2D(
            filters=64,
            kernel_size=filter_size[self.classes],
            activation='relu'
        ),
        keras.layers.Flatten(),
        keras.layers.Dense(
            units=hp.Int('dense_1_units', min_value=32, max_value=512, step=16),
            activation='relu'
        ),
        keras.layers.Dense(43, activation='softmax')
      ])

      model[self.classes].compile(optimizer=keras.optimizers.Adam(hp.Choice('learning_rate', values=[1e-2, 1e-3])),
                  loss='categorical_crossentropy',
                  metrics=['accuracy'])

      return model[self.classes]

"""**Instantiating a Tuner to Search for Best Model**

"""

h = [0]*len(model)
for i in range(len(h)):
  hypermodel = MyHyperModel(classes=i)
  tuner_search=RandomSearch(hypermodel,
                          objective='val_accuracy',
                          overwrite='true',
                          max_trials=5,directory='output',project_name="Traffic")
  # Initiating the Search for the Best Hyperparameter Configuration
  tuner_search.search(data['x_train'],data['y_train'],epochs=5, validation_data=(data['x_validation'], data['y_validation']))
  # Retrieving the Best Model
  model[i]=tuner_search.get_best_models(num_models=1)[0]
  # Model Fitting
  h[i] = model[i].fit(data['x_train'],data['y_train'], epochs=8, validation_data=(data['x_validation'], data['y_validation']), initial_epoch=3)

"""**Initiating the Search for the Best Hyperparameter Configuration**"""

for i in range(len(h)):
  print('Model with Filter '+str(filter_size[i])+'x'+str(filter_size[i]) +' training accuracy={:.5f}, validation accuracy={:.5f}'.\
    format(max(h[i].history['accuracy']), max(h[i].history['val_accuracy'])))

"""**Plotting Comparison of Different Dimensions of Convolution Layer Filters based on their Accuracy**"""

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline
plt.rcParams['figure.figsize'] = (15.0, 15.0) # Setting default size of plots
plt.rcParams['image.interpolation'] = 'nearest'
plt.rcParams['font.family'] = 'Times New Roman'

# Plotting history of training accuracy
fig = plt.figure()
plt.subplot(2, 1, 1)
plt.plot(h[7].history['accuracy'], '-s', linewidth=3.0)
plt.plot(h[6].history['accuracy'], '-D', linewidth=3.0)
plt.plot(h[5].history['accuracy'], '-o', linewidth=3.0)
plt.plot(h[4].history['accuracy'], '-s', linewidth=3.0)
plt.plot(h[3].history['accuracy'], '-D', linewidth=3.0)
plt.plot(h[2].history['accuracy'], '-o', linewidth=3.0)
plt.plot(h[1].history['accuracy'], '-s', linewidth=3.0)
plt.plot(h[0].history['accuracy'], '-D', linewidth=3.0)
plt.legend(['filter 19', 'filter 15', 'filter 13','filter 11', 'filter 9','filter 7', 'filter 5', 'filter 3'], loc='lower right', fontsize='xx-large', borderpad=2)
plt.xlabel('Epoch', fontsize=20, fontname='Times New Roman')
plt.ylabel('Training Accuracy', fontsize=20, fontname='Times New Roman')
plt.yscale('linear')  # {"linear", "log", "symlog", "logit", ...}
plt.ylim(0.94, 1.0)
plt.xlim(0.5, 5.3) 
plt.title('Accuracy for different sizes of filters', fontsize=22)
plt.tick_params(labelsize=18)

plt.subplot(2, 1, 2)
plt.plot(h[7].history['val_accuracy'], '-s', linewidth=3.0)
plt.plot(h[6].history['val_accuracy'], '-D', linewidth=3.0)
plt.plot(h[5].history['val_accuracy'], '-o', linewidth=3.0)
plt.plot(h[4].history['val_accuracy'], '-s', linewidth=3.0)
plt.plot(h[3].history['val_accuracy'], '-D', linewidth=3.0)
plt.plot(h[2].history['val_accuracy'], '-o', linewidth=3.0)
plt.plot(h[1].history['val_accuracy'], '-s', linewidth=3.0)
plt.plot(h[0].history['val_accuracy'], '-D', linewidth=3.0)
plt.legend(['filter 19', 'filter 15', 'filter 13','filter 11', 'filter 9','filter 7', 'filter 5', 'filter 3'], loc='lower right', fontsize='xx-large', borderpad=2)
plt.xlabel('Epoch', fontsize=20, fontname='Times New Roman')
plt.ylabel('Validation Accuracy', fontsize=20, fontname='Times New Roman')
plt.yscale('linear')  # {"linear", "log", "symlog", "logit", ...}
plt.ylim(0.86, 0.97)
plt.xlim(0.5, 5.3)
plt.tick_params(labelsize=18)

# Showing the plot
plt.show()

# Saving the plot
fig.savefig('models_accuracy.png')
plt.close()


# Showing values of accuracy for different filters
for i in range(len(h)):
    print('For the filter '+str(filter_size[i])+'x'+str(filter_size[i])+' training accuracy = {:.5f}'.\
          format( np.max(h[i].history['accuracy'])))

print()

for i in range(len(h)):
    print('For the filter '+str(filter_size[i])+'x'+str(filter_size[i])+' validation accuracy = {:.5f}'.\
          format( np.max(h[i].history['val_accuracy'])))

"""**Calculating Accuracy Based on Testing Dataset**"""

for i in range(len(model)):
    temp = model[i].predict(data['x_test'])
    temp = np.argmax(temp, axis=1)

    # We compare predicted class with correct class for all input images
    # And calculating mean value among all values of following numpy array
    # By saying 'testing_accuracy == data['y_test']' we create numpy array with True and False values
    # 'np.mean' function will return average of the array elements
    # The average is taken over the flattened array by default
    temp = np.mean(temp == data['y_test'])
    
    print('data2 '+str(filter_size[i])+' testing accuracy = {:.5f}'.format(temp))

"""**Classification rate for each Filter Dimension in Seconds**"""

# Getting scores from forward pass of one input image
# Scores are given for each image with 43 numbers of predictions for each class
# Measuring at the same time execution time

for i in range(len(model)):
    start = timer()
    temp = model[i].predict(data['x_test'][:1, :, :, :])
    end = timer()
    
    print('data2 filter {0:d} classification time = {1:.5f}'.format(filter_size[i], end - start))

"""**Visualizing Filters of Convolution Layer**"""

for i in range(len(model)):
    w = model[i].get_weights()
    print(w[0].shape)
    # print(model[i].get_config())
    # l = model[i].layers
    # print(l[0].get_weights()[0].shape)

    # Visualizing filters
    temp = w[0].transpose(3, 0, 1, 2)
    print(temp.shape)  # (81, 32, 32, 3)

    # Plotting
    fig = plt.figure()
    grid = convert_to_grid(temp)
    plt.imshow(grid.astype('uint8'), cmap='gray')
    plt.axis('off')
    plt.gcf().set_size_inches(10, 10)
    name = 'Trained filters ' + str(filter_size[i]) + 'x' + str(filter_size[i])
    plt.title(name, fontsize=18)
    
    # Showing the plot
    plt.show()

    # Saving the plot
    name = 'filters-' + str(filter_size[i]) + 'x' + str(filter_size[i]) + '.png'
    fig.savefig(name)
    plt.close()

"""**Predicting with One Image from Dataset**"""

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline

# Preparing image for predicting from test dataset
x_input = data['x_test'][311:312]
print(x_input.shape)
y_input = data['y_test'][311:312]
print(y_input)

plt.rcParams['figure.figsize'] = (2.5, 2.5) # Setting default size of plots
plt.imshow(x_input[0, :, :, :])
plt.axis('off')

# Showing the plot
plt.show()

# Getting scores from forward pass of input image
#scores = model[0].predict(x_input)
scores = model[1].predict(x_input)
print(scores[0].shape) # (43,)

# Scores is given for image with 43 numbers of predictions for each class
# Getting only one class with maximum value
prediction = np.argmax(scores)
print('ClassId:', prediction)
# Defining function for getting texts for every class - labels
def label_text(file):
    # Defining list for saving label in order from 0 to 42
    label_list = []
    
    # Reading 'csv' file and getting image's labels
    r = pd.read_csv(file)
    # Going through all names
    for name in r['SignName']:
        # Adding from every row second column with name of the label
        label_list.append(name)
    
    # Returning resulted list with labels
    return label_list


# Getting labels
labels = label_text('/content/label_names.csv')

# Printing label for classified Traffic Sign
print('Label:', labels[prediction])

for i in range(len(model)):
    name = 'model-' + str(filter_size[i]) + 'x' + str(filter_size[i]) + '.h5'
    model[i].save(name)